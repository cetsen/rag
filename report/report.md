
# RAG with Hierarchical Chunking for NarrativeQA

## Project Goal and Scope
This project aims to demonstrate and evaluate RAG system for question-answering on a single book, i.e., 'The Time Machine' by H.G. Wells (Project Gutenberg ID 35). The goal is to enhance the accuracy and relevance of answers generated by an LLM by providing it with retrieved context from the book.

## Methodology

### Data Preprocessing
The raw text of 'The Time Machine' was downloaded from Project Gutenberg. It was then cleaned by removing the standard Project Gutenberg headers and footers, and normalizing line breaks (reducing multiple newlines to at most two). The cleaned text was saved to `data/clean/book_clean.txt`.

### Hierarchical Chunking
To prepare the text for retrieval, a hierarchical chunking strategy was used. This involves creating larger "parent" chunks and smaller "child" chunks. This method helps in capturing both broader contextual information (in parent chunks) and more specific, detailed passages (in child chunks). The parameters used were:
- **Parent Chunk Size (`PARENT_SZ`):** 1200 tokens
- **Parent Chunk Overlap (`PARENT_OV`):** 150 tokens
- **Child Chunk Size (`CHILD_SZ`):** 300 tokens
- **Child Chunk Overlap (`CHILD_OV`):** 50 tokens

### Embedding Model
Embeddings for the child chunks were generated using the `sentence-transformers/all-MiniLM-L6-v2` model. This model converts text chunks into dense vector representations, allowing for semantic similarity searches.

### Vector Database
Qdrant was used as an on-disk vector database to store and index the embeddings of the child chunks. It can manage and query vector embeddings efficiently, utilize on-disk storage for data persistence, minimizing memory footprint. This was crucial as the aim of this case study was focused on the retrieval quality and generation accuracy under simulated resource constraints (Google Colab free tier).

### Retrieval Strategy
The `retrieve` function implements a multi-stage retrieval process:
1.  The user's question is embedded into a vector using the same `all-MiniLM-L6-v2` embedding model.
2.  A similarity search is performed in the Qdrant vector database to find the top `K_CHILD` (set to 6) most similar child chunks. This number allows us to cast a reasonably wide net for relevant snippets.
3.  From these top child chunks, we identify the unique parent chunks they originate from. We prioritize `TOP_PARENTS` (`3`) to ensure we get a good breadth of context surrounding the initial child hits.
4.  Context is assembled by concatenating the text of these identified parent chunks, ensuring the total context length does not exceed a `ctx_budget_tokens` (set to 1400 tokens).
This hierarchical approach ensures that the LLM receives both highly relevant specific information (from child chunks guiding parent selection) and a broader context (from the parent chunks).

### LLM Generator and Prompt Design
The RAG system utilizes the `google/gemma-3-1b-it` model as the LLM for generating answers. `make_prompt` function is used to structure the input to the LLM, explicitly instructing the model to:
- Act as a helpful assistant answering questions about a book.
- Only use the provided context to answer the question.
- State that it doesn't know the answer if it's not found in the context.

This structured prompting is crucial for ensuring the LLM adheres to the retrieved information and avoids hallucination.

## Implementation Details

### Key Libraries
The primary Python libraries used in this project include:
-   `transformers`: For loading and using the Gemma LLM.
-   `accelerate`, `safetensors`: Dependencies for `transformers`.
-   `sentence_transformers`: For generating embeddings.
-   `qdrant-client`: For interacting with the Qdrant vector database.
-   `tiktoken`: For tokenizing text to manage context budget.
-   `sacrebleu`, `rouge-score`: For evaluating generation quality.
-   `datasets`: For loading the NarrativeQA dataset.
-   `pandas`: For data manipulation.
-   `huggingface_hub`: For Hugging Face authentication.

### Challenges
Implementing a RAG system involves several considerations:
-   **Managing Token Limits:** Ensuring the retrieved context, combined with the question and prompt, fit within the Gemma model's context window was a constant consideration. Our `ctx_budget_tokens` parameter and token counting (`tiktoken`) were critical for this.
-   **Balancing Precision/Recall in Retrieval:** The `K_CHILD` and `TOP_PARENTS` parameters need tuning to retrieve enough relevant information without overwhelming the LLM with irrelevant data.
-   **Model Hallucinations:** Even with RAG, LLMs can sometimes generate information not present in the provided context, requiring careful prompt engineering.

## Results and Analysis

### Evaluation Table
| Approach | BLEU-4 | ROUGE-L |
|---|---:|---:|
| Baseline | 0.11 | 5.11 |
| RAG | 0.21 | 8.81 |

### Analysis
As shown in the evaluation table, the RAG system significantly outperforms the baseline (No RAG) approach across both BLEU-4 and ROUGE-L metrics. The BLEU-4 score for RAG is almost double that of the baseline, and the ROUGE-L score shows a substantial improvement as well. This indicates that grounding the LLM with relevant, retrieved context through our hierarchical chunking strategy leads to more accurate, relevant, and coherent answers. The RAG approach effectively leverages external knowledge, allowing the smaller Gemma-3-1b-it model to perform better on a specific domain task than it would in a zero-shot setting.

### Resource Usage
The LLM was loaded with `device_map='auto'`, leveraging available GPU resources for efficient inference. The Qdrant vector database was configured to run on-disk, providing persistence and reducing memory overhead during operation.

## Conclusion and Future Improvements

This project successfully implemented a RAG system with hierarchical chunking for question-answering on 'The Time Machine'. The results clearly demonstrate that integrating a retrieval mechanism dramatically improves the performance of the LLM in answering questions based on the provided text. This approach is highly viable for similar QA tasks where grounding the LLM's responses in specific source material is essential.

For future work, several improvements could be explored:
-   **Alternative Embedding Models:** Experimenting with different or more powerful embedding models.
-   **Refining Chunking Strategies:** Further optimizing parent/child chunk sizes and overlaps, or exploring other chunking methodologies.
-   **Different LLMs:** Testing various LLMs (e.g., larger Gemma models, Llama 3) to observe their interaction with the RAG system.
-   **Re-ranking:** Implementing a re-ranking step after initial retrieval to further improve the relevance of retrieved chunks.
-   **More Sophisticated Context Assembly:** Exploring techniques beyond simple concatenation for combining parent chunks, such as summarization or weighting.
