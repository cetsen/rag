{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sIsBEy7wSdvU",
   "metadata": {
    "id": "sIsBEy7wSdvU"
   },
   "source": [
    "## 1 — Imports & constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ntCfouXRbtOI",
   "metadata": {
    "id": "ntCfouXRbtOI"
   },
   "outputs": [],
   "source": [
    "!pip install -U \"fsspec[http]==2024.6.1\" \"gcsfs==2024.6.1\" \"protobuf<6\"\n",
    "!pip install -U transformers accelerate safetensors huggingface_hub\n",
    "!pip install -U qdrant-client==1.9.1 sentence-transformers==3.2.1\n",
    "!pip install -U sacrebleu==2.4.2 rouge-score==0.1.2 tiktoken==0.7.0 psutil==6.0.0 datasets==3.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4LKBUXC9S-e6",
   "metadata": {
    "id": "4LKBUXC9S-e6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "from huggingface_hub import login\n",
    "from getpass import getpass\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "import tiktoken\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u6uB53u1Z1kP",
   "metadata": {
    "id": "u6uB53u1Z1kP"
   },
   "outputs": [],
   "source": [
    "BOOK_ID = 35\n",
    "VDB_PATH = \"./vdb\"\n",
    "K_CHILD = 6\n",
    "TOP_PARENTS = 3\n",
    "PARENT_SZ, PARENT_OV = 1200, 150\n",
    "CHILD_SZ, CHILD_OV = 300, 50\n",
    "\n",
    "os.makedirs(\"data/clean\", exist_ok=True)\n",
    "os.makedirs(\"data/narrativeqa\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C9lDHmQ4SdvX",
   "metadata": {
    "id": "C9lDHmQ4SdvX"
   },
   "source": [
    "## 2 — Download & clean the Gutenberg book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcbec3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90fcbec3",
    "outputId": "ec6339a9-e67a-4be9-9640-3e36be9e3555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/clean/book_clean.txt | chars: 179621\n"
     ]
    }
   ],
   "source": [
    "url = f\"https://www.gutenberg.org/cache/epub/{BOOK_ID}/pg{BOOK_ID}.txt\"\n",
    "raw = requests.get(url).text\n",
    "\n",
    "# Remove Gutenberg header and footer\n",
    "start = re.search(r\"\\*\\*\\* START OF.*?\\*\\*\\*\", raw, re.I)\n",
    "end   = re.search(r\"\\*\\*\\* END OF.*?\\*\\*\\*\", raw, re.I)\n",
    "text  = raw[(start.end() if start else 0):(end.start() if end else len(raw))]\n",
    "text  = re.sub(r\"\\r\",\"\",text)\n",
    "text  = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
    "open(\"data/clean/book_clean.txt\",\"w\", encoding=\"utf-8\").write(text)\n",
    "\n",
    "print(\"Saved:\", \"data/clean/book_clean.txt\", \"| chars:\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7LDz2EtJSdvc",
   "metadata": {
    "id": "7LDz2EtJSdvc"
   },
   "source": [
    "## 3 — NarrativeQA filter for the chosen title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33cc6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe33cc6f",
    "outputId": "a202bfe2-b9a2-4b8e-9981-493c169e3a7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 1 document(s) with Gutenberg ID 35\n",
      "Found 29 QA pairs linked to those docs\n",
      "Saved 29 QA items to data/narrativeqa/test.json for 'The Time Machine' (Gutenberg ID 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3955871677.py:7: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask = is_test & docs[\"story_url\"].fillna(\"\").str.contains(r\"/ebooks/35(\\.txt|\\.utf-8|$)\", case=False)\n"
     ]
    }
   ],
   "source": [
    "# Load NarrativeQA metadata\n",
    "docs = pd.read_csv(\"https://raw.githubusercontent.com/google-deepmind/narrativeqa/master/documents.csv\")\n",
    "qaps = pd.read_csv(\"https://raw.githubusercontent.com/google-deepmind/narrativeqa/master/qaps.csv\")\n",
    "\n",
    "# Find entries using Gutenberg URL\n",
    "is_test = docs[\"set\"].str.lower().eq(\"test\")\n",
    "mask = is_test & docs[\"story_url\"].fillna(\"\").str.contains(r\"/ebooks/35(\\.txt|\\.utf-8|$)\", case=False)\n",
    "\n",
    "doc_ids = docs.loc[mask, \"document_id\"].unique()\n",
    "print(f\"Matched {len(doc_ids)} document(s) with Gutenberg ID 35\")\n",
    "\n",
    "# Build QA df for those documents\n",
    "qa_df = qaps[(qaps[\"set\"].str.lower() == \"test\") & (qaps[\"document_id\"].isin(doc_ids))].copy()\n",
    "print(f\"Found {len(qa_df)} QA pairs linked to those docs\")\n",
    "\n",
    "# Helper to pick the preferred answer\n",
    "def pick_answer(row):\n",
    "    a1 = str(row.get(\"answer1\") or \"\").strip()\n",
    "    a2 = str(row.get(\"answer2\") or \"\").strip()\n",
    "    return a1 if a1 else a2\n",
    "\n",
    "# Build the QA list\n",
    "qa = [\n",
    "    {\"qid\": int(i), \"question\": str(r[\"question\"]), \"answer\": pick_answer(r)}\n",
    "    for i, r in qa_df.iterrows()\n",
    "    if str(r.get(\"question\",\"\")).strip()\n",
    "]\n",
    "\n",
    "# Save to JSON\n",
    "os.makedirs(\"data/narrativeqa\", exist_ok=True)\n",
    "path = \"data/narrativeqa/test.json\"\n",
    "with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(qa)} QA items to {path} for 'The Time Machine' (Gutenberg ID 35)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xcFQzGBpSdv6",
   "metadata": {
    "id": "xcFQzGBpSdv6"
   },
   "source": [
    "## 4 — Hierarchical chunking (parents & children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3263f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad3263f9",
    "outputId": "2c62fded-e9b6-45bd-fde2-443f6cdf8058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parents: 43 | Children: 215\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "def toklen(s): return len(enc.encode(s))\n",
    "\n",
    "def sliding_windows(tokens, size, overlap):\n",
    "    step = size - overlap\n",
    "    for i in range(0, max(1, len(tokens)-size+1), step):\n",
    "        yield i, tokens[i:i+size]\n",
    "    if len(tokens) > size and (len(tokens)-size) % step != 0:\n",
    "        i = len(tokens)-size\n",
    "        yield i, tokens[i:]\n",
    "\n",
    "tokens = enc.encode(text)\n",
    "parents = []\n",
    "for p_start, p_tok in sliding_windows(tokens, PARENT_SZ, PARENT_OV):\n",
    "    parents.append({\"p_id\": len(parents),\n",
    "                    \"toks\": p_tok,\n",
    "                    \"text\": enc.decode(p_tok),\n",
    "                    \"p_start\": p_start, \"p_end\": p_start+len(p_tok)})\n",
    "\n",
    "children = []\n",
    "for p in parents:\n",
    "    for c_start, c_tok in sliding_windows(p[\"toks\"], CHILD_SZ, CHILD_OV):\n",
    "        children.append({\n",
    "            \"c_id\": len(children),\n",
    "            \"p_id\": p[\"p_id\"],\n",
    "            \"text\": enc.decode(c_tok),\n",
    "        })\n",
    "\n",
    "print(\"Parents:\", len(parents), \"| Children:\", len(children))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sBy70wvGSdv9",
   "metadata": {
    "id": "sBy70wvGSdv9"
   },
   "source": [
    "## 5 — Embeddings + on-disk Qdrant index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Eh5HfNgNSdv-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "04101e89e1904617b28c8541358bc59f",
      "8d0f4d34a4014b5893c3cf9d6a74f1c7",
      "53c0935c93de4dcbb57e6a284d152dd3",
      "03d24865fd51471b87c61bc7c2eb6695",
      "b91fc284f8f64ff0bf2e4e403587eb49",
      "a2b4c8722afd4cc5953660cc6294e74e",
      "4e7fe75220724e6f89548f33413b26dc",
      "b66ea50eb3e44d2498d2765a78b61a62",
      "e1989ab7291a43f9b7ec36625cae655f",
      "c92808c1e8254e1abaef13f7b6f644d7",
      "5c49137dada945868d7994f8ddd402df"
     ]
    },
    "id": "Eh5HfNgNSdv-",
    "outputId": "279874db-624e-4b8a-c3fd-9ac800bbeefd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04101e89e1904617b28c8541358bc59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3230368861.py:12: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qc.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant collection created: gutenberg_35_children\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(VDB_PATH, exist_ok=True)\n",
    "emb = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "X = emb.encode([c[\"text\"] for c in children], batch_size=64, show_progress_bar=True)\n",
    "\n",
    "qc = QdrantClient(path=VDB_PATH)\n",
    "COL = f\"gutenberg_{BOOK_ID}_children\"\n",
    "\n",
    "existing = [c.name for c in qc.get_collections().collections]\n",
    "if COL in existing:\n",
    "    qc.delete_collection(COL)\n",
    "\n",
    "qc.recreate_collection(\n",
    "    collection_name=COL,\n",
    "    vectors_config=qmodels.VectorParams(size=X.shape[1], distance=qmodels.Distance.COSINE)\n",
    ")\n",
    "\n",
    "qc.upsert(\n",
    "    collection_name=COL,\n",
    "    points=[\n",
    "        qmodels.PointStruct(id=i, vector=X[i].tolist(),\n",
    "                            payload={\"c_id\": i, \"p_id\": children[i][\"p_id\"], \"text\": children[i][\"text\"]})\n",
    "        for i in range(len(children))\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Qdrant collection created:\", COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XYd6Mv6ESdwA",
   "metadata": {
    "id": "XYd6Mv6ESdwA"
   },
   "source": [
    "## 6 — Gemma-3-1b-it generator & prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92189c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "referenced_widgets": [
      "1c16453d0f134b039ca153803ca79d69",
      "9445341d818b4506850416a6b973a4c3",
      "d3ade538ebc94def8c48402e1588586c",
      "93278bb1f4234b568bcdcc8bd119243b",
      "ef2750fe4ae9439b9680cbc30b219198",
      "49e1715bc22b4bc48880e34430d927c0",
      "de182b01cb45473fb42a7dae45126658",
      "6ee21e331a2242bb82c4c86e293932f0",
      "7b1e35c3af3e4a5f829061d5e3236725",
      "85b424e4a5ce4bf4be42915f28c584bc",
      "860cf078f53c4981bb81f314241b8bc0",
      "7c48ec3d188a47f3ab8dec603f133745",
      "efb5cd378a4f4b878e934b48bfcbc98e",
      "9a84258534364c0b921b895d782676c0",
      "5a31142920a24debadf81cad0542c152",
      "69f0660abfda4273beaec5d3ca3d97a2",
      "70a95397fe384a45906ed238608b61d5",
      "839b389fb4f14d02a36dbb553951d6b7",
      "179999f4ea0941dfbe025d870fff9ebd",
      "65bec62949ea427c88374aec9f110c33",
      "98953a53362141dd870a375e3223f29d",
      "71fd96a967244cf19e306d7e164391a4"
     ]
    },
    "id": "ab92189c",
    "outputId": "8875c582-36c3-4abf-cf4c-fa8b9791df69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste your HF token (starts with hf_):\n",
      "··········\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c16453d0f134b039ca153803ca79d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c48ec3d188a47f3ab8dec603f133745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Gemma 3: google/gemma-3-1b-it\n",
      "Gemma-3-1b-it generator and make_prompt function defined.\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "else:\n",
    "    print(\"Paste your HF token (starts with hf_):\")\n",
    "    HF_TOKEN = getpass()\n",
    "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN, trust_remote_code=True)\n",
    "mdl = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\", token=HF_TOKEN, trust_remote_code=True)\n",
    "print(\"Loaded Gemma 3:\", MODEL_ID)\n",
    "\n",
    "GEN_KW = dict(max_new_tokens=32, do_sample=False, temperature=0.1, top_p=0.8)\n",
    "\n",
    "gen = pipeline(\"text-generation\", model=mdl, tokenizer=tok)\n",
    "\n",
    "def make_prompt(context, question):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant answering questions about a book.\n",
    "Use ONLY the following context to answer the question. If the answer is not found in the context, state that you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "    return prompt\n",
    "\n",
    "print(\"Gemma-3-1b-it generator and make_prompt function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m_MXiNobSdwC",
   "metadata": {
    "id": "m_MXiNobSdwC"
   },
   "source": [
    "## 7 — Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l5_v9htWSdwD",
   "metadata": {
    "id": "l5_v9htWSdwD"
   },
   "outputs": [],
   "source": [
    "def retrieve(q, k_child=K_CHILD, top_par=TOP_PARENTS, ctx_budget_tokens=1400):\n",
    "    q_vec = emb.encode([q])[0]\n",
    "    res = qc.search(COL, query_vector=q_vec, limit=k_child)\n",
    "    child_hits = [children[p.payload[\"c_id\"]] for p in res]\n",
    "    p_ids = []\n",
    "    for ch in child_hits[:top_par]:\n",
    "        if ch[\"p_id\"] not in p_ids:\n",
    "            p_ids.append(ch[\"p_id\"])\n",
    "    ctx = \"\"\n",
    "    for pid in p_ids:\n",
    "        pt = parents[pid][\"text\"].strip()\n",
    "        if toklen(ctx + \"\\n\\n\" + pt) <= ctx_budget_tokens:\n",
    "            ctx += (\"\\n\\n\" + pt) if ctx else pt\n",
    "        else:\n",
    "            break\n",
    "    return ctx, res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PNS8BjkTSdwD",
   "metadata": {
    "id": "PNS8BjkTSdwD"
   },
   "source": [
    "## 8 — Run predictions (baseline & RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7a387",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0c7a387",
    "outputId": "f490d8ef-b8df-408a-e701-1fd27c9ebda1"
   },
   "outputs": [],
   "source": [
    "def answer_baseline(q):\n",
    "    out = gen(q, **GEN_KW)[0][\"generated_text\"]\n",
    "    return out.split(q)[-1].strip() if q in out else out\n",
    "\n",
    "def answer_rag(q):\n",
    "    ctx, _ = retrieve(q)\n",
    "    prompt = make_prompt(ctx, q)\n",
    "    out = gen(prompt, **GEN_KW)[0][\"generated_text\"]\n",
    "    return out.split(\"Answer:\")[-1].strip() if \"Answer:\" in out else out\n",
    "\n",
    "qa = json.load(open(\"data/narrativeqa/test.json\", encoding=\"utf-8\"))\n",
    "baseline_preds, rag_preds, refs = [], [], []\n",
    "for r in qa:\n",
    "    refs.append(r[\"answer\"])\n",
    "    baseline_preds.append(answer_baseline(r[\"question\"]))\n",
    "    rag_preds.append(answer_rag(r[\"question\"]))\n",
    "\n",
    "json.dump({\"refs\": refs, \"baseline\": baseline_preds, \"rag\": rag_preds},\n",
    "          open(\"outputs_mvp.json\",\"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "print(\"Saved new predictions to outputs_mvp.json with updated parameters and embedding model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GCF2q6IOSdwF",
   "metadata": {
    "id": "GCF2q6IOSdwF"
   },
   "source": [
    "## 9 — BLEU-4 & ROUGE-L summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NjucMNEAf-MV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjucMNEAf-MV",
    "outputId": "6953e3b4-4e18-4e98-fc55-ffb6733a81f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Approach | BLEU-4 | ROUGE-L |\n",
      "|---|---:|---:|\n",
      "| Baseline (No RAG) | 0.11 | 5.11 |\n",
      "| RAG (Hierarchical - Updated) | 0.21 | 8.81 |\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(\"outputs_mvp.json\", encoding=\"utf-8\"))\n",
    "refs, baseline_preds, rag_preds = data[\"refs\"], data[\"baseline\"], data[\"rag\"]\n",
    "\n",
    "bleu_base = sacrebleu.corpus_bleu(baseline_preds, [refs]).score\n",
    "bleu_rag  = sacrebleu.corpus_bleu(rag_preds, [refs]).score\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "def rougeL(preds, refs):\n",
    "    return 100 * np.mean([scorer.score(a,b)['rougeL'].fmeasure for a,b in zip(preds, refs)])\n",
    "\n",
    "rouge_base = rougeL(baseline_preds, refs)\n",
    "rouge_rag  = rougeL(rag_preds, refs)\n",
    "\n",
    "print(\"| Approach | BLEU-4 | ROUGE-L |\")\n",
    "print(\"|---|---:|---:|\")\n",
    "print(f\"| Baseline (No RAG) | {bleu_base:.2f} | {rouge_base:.2f} |\")\n",
    "print(f\"| RAG (Hierarchical - Updated) | {bleu_rag:.2f} | {rouge_rag:.2f} |\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}